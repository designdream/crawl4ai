"""
Rule-based extraction strategy for Crawl4AI that avoids hallucinations.

This module provides extraction strategies based on traditional NLP techniques
rather than generative AI, focusing on deterministic extraction with high confidence.
"""

import re
import json
import logging
import traceback
from typing import List, Dict, Any, Tuple, Optional
from bs4 import BeautifulSoup
from collections import defaultdict

# Import required base classes and utilities
from .extraction_strategy import ExtractionStrategy
from .utils import sanitize_html, normalize_text

# Configure logging
logger = logging.getLogger(__name__)

# Optional: Try to import spaCy for more advanced NLP capabilities
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

class RuleBasedExtractionStrategy(ExtractionStrategy):
    """
    Extract text using rule-based patterns without relying on generative AI.
    
    This strategy uses regex patterns, keyword matching, and contextual rules
    to extract factual information without hallucinations.
    """
    
    def __init__(
        self, 
        patterns: List[Dict] = None,
        confidence_threshold: float = 0.7,
        extract_entities: bool = True,
        extract_relations: bool = True,
        include_context: bool = True,
        context_window: int = 100,
        **kwargs
    ):
        """
        Initialize the rule-based extraction strategy.
        
        Args:
            patterns: List of extraction patterns (regex or keyword-based)
            confidence_threshold: Minimum confidence score to include extractions
            extract_entities: Whether to extract named entities
            extract_relations: Whether to extract relationships between entities
            include_context: Whether to include surrounding context with extractions
            context_window: Number of characters to include as context
            **kwargs: Additional arguments passed to parent ExtractionStrategy
        """
        super().__init__(input_format="html", **kwargs)
        self.patterns = patterns or []
        self.confidence_threshold = confidence_threshold
        self.extract_entities = extract_entities
        self.extract_relations = extract_relations
        self.include_context = include_context
        self.context_window = context_window
        self.verbose = kwargs.get("verbose", False)
        
        # Initialize spaCy if available
        self.nlp = None
        if SPACY_AVAILABLE and self.extract_entities:
            try:
                self.nlp = spacy.load("en_core_web_sm")
                if self.verbose:
                    print("[INFO] Loaded spaCy model for entity extraction")
            except Exception as e:
                if self.verbose:
                    print(f"[WARNING] Failed to load spaCy model: {e}")

    def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]:
        """
        Extract meaningful blocks using rule-based approaches.
        
        Args:
            url: The URL of the webpage
            html: The HTML content of the webpage
            
        Returns:
            List of extraction results with confidence scores
        """
        logger.info(f"ðŸ” Starting rule-based extraction on URL: {url}")
        results = []
        
        try:
            # Check if HTML is empty or None
            if not html:
                logger.error(f"âŒ Empty HTML content for URL: {url}")
                return [{"error": "Empty HTML content", "extraction_method": "rule_based", "url": url, "confidence": 0.0}]
            
            # Clean and parse HTML
            logger.info(f"ðŸ§¹ Cleaning and parsing HTML ({len(html)} bytes)")
            clean_html = sanitize_html(html)
            
            try:
                soup = BeautifulSoup(clean_html, "html.parser")
            except Exception as e:
                logger.error(f"âŒ BeautifulSoup parsing error: {str(e)}")
                return [{"error": f"HTML parsing error: {str(e)}", "extraction_method": "rule_based", "url": url, "confidence": 0.0}]
            
            # Extract plain text from HTML
            text = soup.get_text(separator=" ", strip=True)
            if not text:
                logger.warning(f"âš ï¸ No text content extracted from HTML for URL: {url}")
                return [{"error": "No text content in HTML", "extraction_method": "rule_based", "url": url, "confidence": 0.0}]
                
            logger.info(f"ðŸ“ Extracted {len(text)} characters of text content")
            normalized_text = normalize_text(text)
            
            # 1. Apply regex pattern matching
            logger.info(f"ðŸ”§ Applying pattern matching with {len(self.patterns)} patterns")
            pattern_matches = self._apply_patterns(normalized_text)
            logger.info(f"âœ… Found {len(pattern_matches)} pattern matches")
            
            for match in pattern_matches:
                if match["confidence"] >= self.confidence_threshold:
                    results.append(match)
                    logger.info(f"ðŸŽ¯ Match: {match['type']} - {match['value'][:30]}... (confidence: {match['confidence']:.2f})")
                else:
                    logger.debug(f"ðŸ‘Ž Low confidence match rejected: {match['type']} - {match['value'][:30]}... (confidence: {match['confidence']:.2f})")
            
            # 2. Apply entity extraction if enabled
            if self.extract_entities and self.nlp:
                logger.info(f"ðŸ§© Extracting named entities")
                try:
                    entity_results = self._extract_entities(normalized_text)
                    logger.info(f"âœ… Found {len(entity_results)} entities")
                    
                    for entity in entity_results:
                        if entity["confidence"] >= self.confidence_threshold:
                            results.append(entity)
                            logger.info(f"ðŸ§  Entity: {entity['type']} - {entity['value'][:30]}... (confidence: {entity['confidence']:.2f})")
                        else:
                            logger.debug(f"ðŸ‘Ž Low confidence entity rejected: {entity['type']} - {entity['value'][:30]}... (confidence: {entity['confidence']:.2f})")
                except Exception as e:
                    logger.error(f"âŒ Entity extraction error: {str(e)}")
                    # Continue with other extraction methods
            
            # 3. Apply relation extraction if enabled
            if self.extract_relations and len(results) > 1:
                logger.info(f"ðŸ”„ Extracting relations between {len(results)} items")
                try:
                    relation_results = self._extract_relations(results, normalized_text)
                    logger.info(f"âœ… Found {len(relation_results)} relations")
                    
                    for relation in relation_results:
                        if relation["confidence"] >= self.confidence_threshold:
                            results.append(relation)
                            logger.info(f"ðŸ”— Relation: {relation['type']} - {relation.get('value', '')[:30]}... (confidence: {relation['confidence']:.2f})")
                        else:
                            logger.debug(f"ðŸ‘Ž Low confidence relation rejected: {relation['type']} (confidence: {relation['confidence']:.2f})")
                except Exception as e:
                    logger.error(f"âŒ Relation extraction error: {str(e)}")
                    # Continue with verification
            
            # Add source verification
            logger.info(f"âœ… Adding verification to {len(results)} results")
            results = self._add_verification(results, normalized_text)
            
            # Remove duplicates and sort by confidence
            unique_results = self._deduplicate_results(results)
            logger.info(f"ðŸ§¹ Deduplicated to {len(unique_results)} unique results (from {len(results)})")
            
            sorted_results = sorted(unique_results, key=lambda x: x.get("confidence", 0), reverse=True)
            
            # Add metadata
            for result in sorted_results:
                result["url"] = url
                result["extraction_method"] = "rule_based"
            
            logger.info(f"âœ¨ Rule-based extraction completed with {len(sorted_results)} high-confidence results")
            return sorted_results
            
        except Exception as e:
            error_msg = f"Extraction error: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"ðŸ§¨ Traceback: {traceback.format_exc()}")
            return [{"error": error_msg, "extraction_method": "rule_based", "url": url, "confidence": 0.0}]
    
    def _apply_patterns(self, text: str) -> List[Dict[str, Any]]:
        """Apply regex and keyword patterns to extract information."""
        results = []
        
        try:
            # Apply custom patterns if provided
            if self.patterns:
                logger.info(f"ðŸ§® Applying {len(self.patterns)} custom patterns")
                for pattern_dict in self.patterns:
                    pattern = pattern_dict.get("pattern", "")
                    pattern_type = pattern_dict.get("type", "regex")
                    name = pattern_dict.get("name", "custom_pattern")
                    
                    try:
                        if pattern_type == "regex":
                            try:
                                matches = list(re.finditer(pattern, text))
                                logger.info(f"âœ… Pattern '{name}': Found {len(matches)} matches")
                                
                                for match in matches:
                                    # Get the matched text
                                    match_text = match.group(0)
                                    
                                    # Calculate confidence based on specificity
                                    confidence = self._calculate_confidence(match_text, pattern)
                                    
                                    # Get context if requested
                                    context = self._get_context(text, match.start(), match.end()) if self.include_context else ""
                                    
                                    results.append({
                                        "type": name,
                                        "value": match_text,
                                        "confidence": confidence,
                                        "context": context,
                                        "groups": {k: v for k, v in match.groupdict().items()},
                                        "span": (match.start(), match.end())
                                    })
                            except re.error as e:
                                logger.error(f"âŒ Invalid regex pattern '{pattern}': {e}")
                                continue
                        
                        elif pattern_type == "keyword":
                            try:
                                keyword_matches = self._find_keyword_matches(text, pattern)
                                logger.info(f"âœ… Keyword '{name}': Found {len(keyword_matches)} matches")
                                
                                for start, end in keyword_matches:
                                    match_text = text[start:end]
                                    confidence = self._calculate_confidence(match_text, pattern)
                                    context = self._get_context(text, start, end) if self.include_context else ""
                                    
                                    results.append({
                                        "type": name,
                                        "value": match_text,
                                        "confidence": confidence,
                                        "context": context,
                                        "span": (start, end)
                                    })
                            except Exception as e:
                                logger.error(f"âŒ Error in keyword matching for '{name}': {e}")
                                continue
                    
                    except Exception as e:
                        logger.error(f"âŒ Error processing pattern '{name}': {e}")
                        # Continue with other patterns
            
            # Apply common regulatory patterns if no custom patterns
            if not self.patterns:
                logger.info(f"ðŸ”„ No custom patterns, applying default regulatory patterns")
                default_results = self._apply_default_patterns(text)
                logger.info(f"âœ… Default patterns: Found {len(default_results)} matches")
                results.extend(default_results)
                
            return results
        
        except Exception as e:
            logger.error(f"âŒ Fatal error in pattern application: {e}")
            logger.error(f"ðŸ§¨ Traceback: {traceback.format_exc()}")
            return []
    
    def _apply_default_patterns(self, text: str) -> List[Dict[str, Any]]:
        """Apply default patterns for regulatory information."""
        results = []
        
        # Pattern for CE hours
        ce_hours_pattern = r"(\d+)\s*(?:hours?|hrs?|credit\s*hours?|CE\s*hours?|contact\s*hours?|CEUs?)"
        matches = re.finditer(ce_hours_pattern, text, re.IGNORECASE)
        for match in matches:
            hours = match.group(1)
            match_text = match.group(0)
            context = self._get_context(text, match.start(), match.end()) if self.include_context else ""
            
            results.append({
                "type": "ce_hours",
                "value": hours,
                "raw_match": match_text,
                "confidence": 0.85,  # High confidence for numeric hour matches
                "context": context,
                "span": (match.start(), match.end())
            })
        
        # Pattern for renewal periods
        renewal_pattern = r"(?:renewal|license renewal|renew).{0,30}(?:every|each)\s+(\d+)\s*(years?|months?)"
        matches = re.finditer(renewal_pattern, text, re.IGNORECASE)
        for match in matches:
            period_value = match.group(1)
            period_unit = match.group(2)
            match_text = match.group(0)
            context = self._get_context(text, match.start(), match.end()) if self.include_context else ""
            
            results.append({
                "type": "renewal_period",
                "value": f"{period_value} {period_unit}",
                "raw_match": match_text,
                "confidence": 0.8,
                "context": context,
                "span": (match.start(), match.end())
            })
            
        # Pattern for specialized CE requirements
        specialized_pattern = r"(\d+)\s*(?:hours?|credits?).{0,50}(?:in|of|for)\s+([a-zA-Z\s]{3,50})"
        matches = re.finditer(specialized_pattern, text, re.IGNORECASE)
        for match in matches:
            hours = match.group(1)
            topic = match.group(2).strip()
            match_text = match.group(0)
            context = self._get_context(text, match.start(), match.end()) if self.include_context else ""
            
            results.append({
                "type": "specialized_requirement",
                "hours": hours,
                "topic": topic,
                "raw_match": match_text,
                "confidence": 0.75,
                "context": context,
                "span": (match.start(), match.end())
            })
            
        return results
    
    def _find_keyword_matches(self, text: str, keyword: str) -> List[Tuple[int, int]]:
        """Find all occurrences of a keyword in text."""
        matches = []
        start = 0
        keyword_lower = keyword.lower()
        text_lower = text.lower()
        
        while True:
            start = text_lower.find(keyword_lower, start)
            if start == -1:
                break
            end = start + len(keyword)
            matches.append((start, end))
            start = end
            
        return matches
    
    def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """Extract named entities using spaCy."""
        results = []
        
        if not self.nlp:
            return results
            
        doc = self.nlp(text)
        
        # Process each entity
        for ent in doc.ents:
            # Filter out less relevant entity types
            if ent.label_ in ["DATE", "TIME", "CARDINAL", "ORDINAL", "MONEY", "PERCENT", "QUANTITY"]:
                confidence = 0.75
            else:
                confidence = 0.6
                
            context = self._get_context(text, ent.start_char, ent.end_char) if self.include_context else ""
            
            results.append({
                "type": "entity",
                "entity_type": ent.label_,
                "value": ent.text,
                "confidence": confidence,
                "context": context,
                "span": (ent.start_char, ent.end_char)
            })
            
        return results
    
    def _extract_relations(self, entities: List[Dict], text: str) -> List[Dict[str, Any]]:
        """Extract relationships between entities based on proximity and patterns."""
        results = []
        
        # Simple approach: relate entities that are close to each other
        sorted_entities = sorted(entities, key=lambda x: x.get("span", (0, 0))[0])
        
        for i in range(len(sorted_entities) - 1):
            entity1 = sorted_entities[i]
            entity2 = sorted_entities[i + 1]
            
            # Check if entities are close enough
            _, end1 = entity1.get("span", (0, 0))
            start2, _ = entity2.get("span", (0, 0))
            
            if start2 - end1 <= 50:  # Arbitrary threshold of 50 chars
                between_text = text[end1:start2]
                
                # Look for relation indicators in between text
                relation_indicators = ["of", "for", "in", "requires", "must have"]
                for indicator in relation_indicators:
                    if indicator in between_text.lower():
                        # Create relation
                        relation_text = text[entity1.get("span", (0, 0))[0]:entity2.get("span", (0, 0))[1]]
                        
                        results.append({
                            "type": "relation",
                            "entity1": entity1.get("value"),
                            "entity1_type": entity1.get("type"),
                            "entity2": entity2.get("value"),
                            "entity2_type": entity2.get("type"),
                            "relation": indicator,
                            "text": relation_text,
                            "confidence": 0.65,  # Lower confidence for inferred relations
                            "span": (entity1.get("span", (0, 0))[0], entity2.get("span", (0, 0))[1])
                        })
                        
                        break
        
        return results
        
    def _add_verification(self, results: List[Dict], text: str) -> List[Dict]:
        """Add verification scores based on textual evidence."""
        verified_results = []
        
        for result in results:
            # Initialize verification score based on confidence
            verification_score = result.get("confidence", 0.5)
            
            # Check if the extraction has strong textual evidence
            value = result.get("value", "")
            if isinstance(value, str) and len(value) > 0:
                # Stronger verification for numeric values with clear units
                if re.match(r"^\d+\s*[a-zA-Z]+$", value):
                    verification_score += 0.2
                    
                # Check if value appears multiple times
                occurrences = text.lower().count(value.lower())
                if occurrences > 1:
                    verification_score += min(0.1 * occurrences, 0.2)
                    
                # Higher verification for patterns with named capture groups
                if "groups" in result and result["groups"]:
                    verification_score += 0.1
            
            # Cap verification score at 1.0
            verification_score = min(1.0, verification_score)
            
            # Add verification to result
            result["verification_score"] = verification_score
            result["verified"] = verification_score >= self.confidence_threshold
            
            verified_results.append(result)
            
        return verified_results
    
    def _get_context(self, text: str, start: int, end: int) -> str:
        """Get surrounding context for an extraction."""
        context_start = max(0, start - self.context_window)
        context_end = min(len(text), end + self.context_window)
        
        # Get before and after context
        before = text[context_start:start]
        after = text[end:context_end]
        
        # Clean up context
        before = before.strip()
        after = after.strip()
        
        return f"{before} [EXTRACTION] {after}"
    
    def _calculate_confidence(self, match_text: str, pattern: str) -> float:
        """Calculate confidence score for a match."""
        base_confidence = 0.7
        
        # Adjust confidence based on match characteristics
        if re.search(r"\d+", match_text):
            # Numeric matches often more reliable
            base_confidence += 0.1
            
        # Adjust for specificity of pattern
        pattern_complexity = len(re.findall(r'[()[\]{}|+*?]', pattern)) if isinstance(pattern, str) else 0
        base_confidence += min(0.01 * pattern_complexity, 0.1)
        
        # Adjust for match length (longer matches generally more specific)
        base_confidence += min(0.01 * len(match_text), 0.1)
        
        # Cap at 0.95 (we can never be 100% confident without verification)
        return min(0.95, base_confidence)
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """Remove duplicate extractions."""
        seen_values = set()
        unique_results = []
        
        for result in results:
            # Create a unique key based on type and value
            value = result.get("value", "")
            result_type = result.get("type", "")
            key = f"{result_type}:{value}"
            
            if key not in seen_values:
                seen_values.add(key)
                unique_results.append(result)
            else:
                # If duplicate exists, keep the one with higher confidence
                for i, existing in enumerate(unique_results):
                    existing_type = existing.get("type", "")
                    existing_value = existing.get("value", "")
                    
                    if f"{existing_type}:{existing_value}" == key:
                        if result.get("confidence", 0) > existing.get("confidence", 0):
                            unique_results[i] = result
                        break
        
        return unique_results


class RegulationExtractionStrategy(RuleBasedExtractionStrategy):
    """
    Specialized extraction strategy for healthcare continuing education requirements.
    
    This strategy uses domain-specific patterns and rules to extract structured
    information about CE requirements, renewal periods, and specialized topics.
    """
    
    def __init__(self, **kwargs):
        """Initialize with healthcare regulation specific patterns."""
        patterns = [
            # CE Hours
            {
                "name": "ce_total_hours",
                "type": "regex",
                "pattern": r"(?:required to complete|must complete|need|requires?)\s+(\d+)\s+(?:hours?|contact\s+hours?|continuing\s+education|CE\s+hours?|CEUs?|credits?)"
            },
            # Renewal Period
            {
                "name": "renewal_period",
                "type": "regex",
                "pattern": r"(?:renew|renewal)\s+(?:period|cycle|every|each)\s+(\d+)\s+(years?|months?)"
            },
            # Specialized Topics
            {
                "name": "specialized_topic",
                "type": "regex",
                "pattern": r"(\d+)\s+(?:hours?|credits?).{0,30}(?:in|of|for|related\s+to)\s+([a-zA-Z\s\-]+(?:ethics|safety|law|infection control|substance abuse|pharmacology|medication|emergency|mandatory|required))"
            },
            # Deadlines
            {
                "name": "renewal_deadline",
                "type": "regex",
                "pattern": r"(?:renewal|license).{0,20}(?:deadline|due|expires?|must\s+be\s+completed)\s+(?:by|before|prior\s+to)?\s+([a-zA-Z]+\s+\d{1,2}(?:st|nd|rd|th)?(?:,?\s+\d{4})?)"
            },
            # Exemptions
            {
                "name": "exemption",
                "type": "regex",
                "pattern": r"(?:exempt|exemption|waive|waiver).{0,50}(?:from|for)\s+([a-zA-Z\s\-]+)"
            }
        ]
        
        super().__init__(
            patterns=patterns,
            confidence_threshold=0.7,
            extract_entities=True,
            extract_relations=True,
            include_context=True,
            context_window=150,
            **kwargs
        )
    
    def extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]:
        """
        Extract healthcare regulation information from HTML.
        
        Returns structured data about CE requirements with verification.
        """
        logger.info(f"ðŸ¥ Starting healthcare regulation extraction for URL: {url}")
        
        try:
            # Get basic extractions using parent method
            results = super().extract(url, html, *q, **kwargs)
            
            # Check for errors in base extraction
            if len(results) == 1 and "error" in results[0]:
                logger.error(f"âŒ Base extraction failed: {results[0]['error']}")
                return results
                
            logger.info(f"âœ… Base extraction completed with {len(results)} results")
            
            try:
                # Organize extractions into a structured format
                structured_data = self._structure_regulations(results)
                logger.info(f"ðŸ“‹ Structured regulation data with {len(structured_data.get('specialized_requirements', []))} specialized requirements")
                
                # Calculate confidence score
                confidence = self._calculate_overall_confidence(results)
                logger.info(f"ðŸŽ¯ Overall extraction confidence: {confidence:.2f}")
                
                # Return both the structured format and the raw extractions
                return [{
                    "url": url,
                    "extraction_method": "regulation_extraction",
                    "structured_data": structured_data,
                    "raw_extractions": results,
                    "confidence": confidence
                }]
            except Exception as e:
                logger.error(f"âŒ Error structuring regulation data: {str(e)}")
                logger.error(f"ðŸ§¨ Traceback: {traceback.format_exc()}")
                
                # Return basic results if structuring fails
                return [{
                    "url": url,
                    "extraction_method": "regulation_extraction",
                    "error": f"Structuring error: {str(e)}",
                    "raw_extractions": results,
                    "confidence": 0.5  # Medium confidence since we have raw results
                }]
                
        except Exception as e:
            error_msg = f"Regulation extraction error: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"ðŸ§¨ Traceback: {traceback.format_exc()}")
            return [{"error": error_msg, "extraction_method": "regulation_extraction", "url": url, "confidence": 0.0}]
    
    def _structure_regulations(self, results: List[Dict]) -> Dict[str, Any]:
        """Organize extraction results into structured regulation data."""
        structured = {
            "total_hours": None,
            "renewal_period": None,
            "specialized_requirements": [],
            "deadlines": [],
            "exemptions": [],
            "other_requirements": []
        }
        
        # Process each extraction
        for result in results:
            result_type = result.get("type", "")
            
            if result_type == "ce_total_hours" and structured["total_hours"] is None:
                structured["total_hours"] = {
                    "hours": result.get("value", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                }
            
            elif result_type == "renewal_period" and structured["renewal_period"] is None:
                structured["renewal_period"] = {
                    "period": result.get("value", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                }
            
            elif result_type == "specialized_topic":
                structured["specialized_requirements"].append({
                    "hours": result.get("hours", result.get("value", "")),
                    "topic": result.get("topic", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                })
            
            elif result_type == "renewal_deadline":
                structured["deadlines"].append({
                    "deadline": result.get("value", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                })
            
            elif result_type == "exemption":
                structured["exemptions"].append({
                    "exemption": result.get("value", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                })
            
            # Add any entity that might be related to regulations
            elif result_type == "entity" and result.get("entity_type") in ["ORG", "LAW", "DATE"]:
                structured["other_requirements"].append({
                    "type": result.get("entity_type", ""),
                    "value": result.get("value", ""),
                    "context": result.get("context", ""),
                    "confidence": result.get("confidence", 0),
                    "verified": result.get("verified", False)
                })
        
        return structured
    
    def _calculate_overall_confidence(self, results: List[Dict]) -> float:
        """Calculate overall confidence score for the extraction."""
        if not results:
            return 0.0
            
        # Weight verified results more heavily
        verified_scores = [r.get("confidence", 0) for r in results if r.get("verified", False)]
        unverified_scores = [r.get("confidence", 0) for r in results if not r.get("verified", False)]
        
        if verified_scores:
            verified_avg = sum(verified_scores) / len(verified_scores)
            weight = 0.8
        else:
            verified_avg = 0
            weight = 0
            
        if unverified_scores:
            unverified_avg = sum(unverified_scores) / len(unverified_scores)
            unverified_weight = 1 - weight
        else:
            unverified_avg = 0
            unverified_weight = 0
            
        overall = (verified_avg * weight) + (unverified_avg * unverified_weight)
        
        # Adjust based on number of extractions (more extractions = more confidence)
        extraction_bonus = min(0.1, 0.01 * len(results))
        
        return min(0.95, overall + extraction_bonus)
